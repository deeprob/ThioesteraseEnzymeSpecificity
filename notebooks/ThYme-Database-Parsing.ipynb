{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from Bio import Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_htmldoc(url):\n",
    "    r = requests.get(url)\n",
    "    if r.status_code==200:\n",
    "        return r.text\n",
    "    else:\n",
    "        raise ValueError(f'status code:{r.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_header(tag):                \n",
    "    return tag.get_text().strip()\n",
    "\n",
    "def get_text_rows(tag):\n",
    "    accession_class = ['genbank','ref','uniprot']\n",
    "    if tag['class'][0] in accession_class:\n",
    "        if tag.text:\n",
    "            # find if there are multiple accession numbers\n",
    "            get_a_tags = tag.find_all('a')\n",
    "            return ','.join([a_tag['href'] for a_tag in get_a_tags])\n",
    "                \n",
    "    return tag.get_text().strip()\n",
    "\n",
    "def get_headers(tag):\n",
    "    header_coltags = tag.find_all('th')\n",
    "    return tuple(map(get_text_header,header_coltags))\n",
    "\n",
    "def get_rows(tag):\n",
    "    row_coltags = tag.find_all('td')\n",
    "    return tuple(map(get_text_rows,row_coltags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(html_doc):\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    main_table = soup.find_all('table')[1]\n",
    "    table_rows = main_table.find_all('tr')[1:]\n",
    "    header_row = table_rows[0]\n",
    "    content_rows = table_rows[1:]\n",
    "    header = get_headers(header_row)\n",
    "    all_rows = list(map(get_rows,content_rows))\n",
    "    assert list(map(len,all_rows)) == [len(header) for i in range(len(all_rows))]\n",
    "    df = pd.DataFrame(all_rows,columns=header).replace('','missing')\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fasta(fasta):\n",
    "    header = []\n",
    "    sequences = []\n",
    "    curr_sequence = ''\n",
    "    for lines in fasta:\n",
    "        if lines.startswith('>'):\n",
    "            header.append(lines)\n",
    "            if curr_sequence:\n",
    "                sequences.append(curr_sequence)\n",
    "                curr_sequence = ''\n",
    "        else:\n",
    "            curr_sequence += lines\n",
    "\n",
    "    sequences.append(curr_sequence)\n",
    "    return header,sequences\n",
    "\n",
    "def get_seq_genbank(genbank_url):\n",
    "    pattern = re.compile('^http://www.ncbi.nlm.nih.gov/protein/(.+)')\n",
    "    m = re.match(pattern,genbank_url)\n",
    "    genbank_id = m.group(1)\n",
    "    Entrez.email = 'dzb5732@psu.edu'\n",
    "    handle = Entrez.efetch(db='protein',id=genbank_id,rettype='fasta')\n",
    "    fasta_file = []\n",
    "    for lines in handle:\n",
    "        fasta_file.append(lines.strip())\n",
    "    handle.close()\n",
    "    fasta_header,fasta_seq = parse_fasta(fasta_file)\n",
    "    if len(fasta_header) == len(fasta_seq) == 1:\n",
    "        return fasta_seq[0]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_seq_uniprot(uniprot_url):\n",
    "    url = uniprot_url+'.fasta'\n",
    "    r = requests.get(url)\n",
    "    fasta_file = r.text.strip().split('\\n')\n",
    "    fasta_header,fasta_seq = parse_fasta(fasta_file)\n",
    "    #check if a single fasta file has been returned\n",
    "    if len(fasta_header) == len(fasta_seq) == 1:\n",
    "        return fasta_seq[0]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_seq_refseq(refseq_url):\n",
    "    pattern = re.compile('^http://www.ncbi.nlm.nih.gov/protein/(.+)')\n",
    "    m = re.match(pattern,refseq_url)\n",
    "    refseq_id = m.group(1)\n",
    "    Entrez.email = 'dzb5732@psu.edu'\n",
    "    handle = Entrez.efetch(db='protein',id=refseq_id,rettype='fasta')\n",
    "    fasta_file = []\n",
    "    for lines in handle:\n",
    "        fasta_file.append(lines.strip())\n",
    "    handle.close()\n",
    "    fasta_header,fasta_seq = parse_fasta(fasta_file)\n",
    "    if len(fasta_header) == len(fasta_seq) == 1:\n",
    "        return fasta_seq[0]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_prot_seq(row):\n",
    "    if row['UniProt'] != 'missing':\n",
    "        urls = row['UniProt'].split(',')\n",
    "        for url in urls:\n",
    "            seq = get_seq_uniprot(url)\n",
    "            if seq:\n",
    "                return seq\n",
    "            \n",
    "    if row['GenBank ID'] != 'missing':\n",
    "        urls = row['GenBank ID'].split(',')\n",
    "        for url in urls:\n",
    "            seq = get_seq_genbank(url)\n",
    "            if seq:\n",
    "                return seq\n",
    "\n",
    "    if row['RefSeq'] != 'missing':\n",
    "        urls = row['RefSeq'].split(',')\n",
    "        for url in urls:\n",
    "            seq = get_seq_refseq(url)\n",
    "            if seq:\n",
    "                return seq\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    url_page5 = 'http://www.enzyme.cbirc.iastate.edu/?a=view&c=sequencegroup&id=31&sg_sort_column=&sg_sort_order=1&sg_page=5'\n",
    "    url_page6 = 'http://www.enzyme.cbirc.iastate.edu/?a=view&c=sequencegroup&id=31&sg_sort_column=&sg_sort_order=1&sg_page=6'\n",
    "    html_doc_p5 = get_htmldoc(url_page5)\n",
    "    html_doc_p6 = get_htmldoc(url_page6)\n",
    "    df_p5 = parse_html(html_doc_p5)\n",
    "    df_p6 = parse_html(html_doc_p6)\n",
    "    df_p5['AA_Sequence'] = df_p5.apply(get_prot_seq,axis=1)\n",
    "    df_p6['AA_Sequence'] = df_p6.apply(get_prot_seq,axis=1)\n",
    "    df = pd.concat((df_p5,df_p6))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    df = main()\n",
    "    df.to_csv('../data/thyme/thyme_dataset.csv',columns=['Sequence','Organism','AA_Sequence'],header=False,index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

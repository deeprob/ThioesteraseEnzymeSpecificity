{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append('../')\n",
    "from ensemble.model import Ensemble\n",
    "from baseModels.SVM.model import SVM\n",
    "from featEngg.online.kmerMethods.models import ngModel,gaangModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEClassification:\n",
    "    \n",
    "    def __init__(self,enzseqdata,testenzseqdata,labelfile,trainfeaturefiledirs,testfeaturefiledirs,\n",
    "                 hyperparamfile,random_seed=None,n_models=17,validation_fraction=0.25):\n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        self.n_models = n_models\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.test = True if testfeaturefiledirs else False\n",
    "        \n",
    "                \n",
    "        # original data based on which everything is obtained\n",
    "        df1 = pd.read_csv(enzseqdata,header=None)\n",
    "        df2 = pd.read_csv(labelfile,header=None)\n",
    "        self.train_df = df1.merge(df2,on=0)\n",
    "        \n",
    "        self.enz_names = self.train_df[0].values\n",
    "        self.X = self.train_df.iloc[:,1].values\n",
    "        self.y = self.train_df.iloc[:,-1].values\n",
    "        \n",
    "        # training and validation data for general use\n",
    "        self.X_train, self.X_valid, self.y_train, self.y_valid,self.enz_train,self.enz_valid = train_test_split(self.X, self.y,self.enz_names, test_size=self.validation_fraction, random_state=self.random_seed)\n",
    "        \n",
    "        self.label_file = labelfile\n",
    "        \n",
    "        # test data\n",
    "        if self.test:\n",
    "            self.test_df = pd.read_csv(testenzseqdata,header=None)\n",
    "            self.testenz_names = self.test_df[0].values\n",
    "            self.X_test = self.test_df.iloc[:,1].values\n",
    "        else:\n",
    "            self.X_test=None\n",
    "            \n",
    "        self.df_hyperparam = pd.read_csv(hyperparamfile).set_index('feat_name')\n",
    "        \n",
    "        # kmer and gaakmer\n",
    "        ng = ngModel(self.X_train,self.X_valid,self.X_test)\n",
    "        gaang = gaangModel(self.X_train,self.X_valid,self.X_test)\n",
    "        kmernames = ['kmer','gaakmer']\n",
    "        kmerObjs = [self.get_model_online(ng.Xtrain,ng.Xvalid,self.y_train,self.y_valid,ng.Xtest),self.get_model_online(gaang.Xtrain,gaang.Xvalid,self.y_train,self.y_valid,gaang.Xtest)]\n",
    "\n",
    "        \n",
    "        #generate a list of names from the directories\n",
    "        trainfeatfiles = [d+f.name for d in trainfeaturefiledirs for f in os.scandir(d) if f.name.endswith('.csv.gz')]            \n",
    "        self.featnames = [f.name.replace('.csv.gz','') for d in trainfeaturefiledirs for f in os.scandir(d) if f.name.endswith('.csv.gz')]\n",
    "        \n",
    "        if self.test:\n",
    "            testfeatfiles = [d+f.name for d in testfeaturefiledirs for f in os.scandir(d) if f.name.endswith('.csv.gz')]\n",
    "            func_iter = list(zip(trainfeatfiles,self.featnames,testfeatfiles))\n",
    "            assert [f.name for d in trainfeaturefiledirs for f in os.scandir(d) if f.name.endswith('.csv.gz')]==[f.name for d in testfeaturefiledirs for f in os.scandir(d) if f.name.endswith('.csv.gz')]\n",
    "            self.objects=list(itertools.starmap(self.get_model_offline,func_iter))\n",
    "\n",
    "        else:\n",
    "            # getting all SVM objects together\n",
    "            func_iter = list(zip(trainfeatfiles,self.featnames))\n",
    "            self.objects = list(itertools.starmap(self.get_model_offline,func_iter))\n",
    "            \n",
    "        self.featnames.extend(kmernames)\n",
    "        self.objects.extend(kmerObjs)\n",
    "            \n",
    "        \n",
    "        # select only the best models based on training or validation\n",
    "        self.best_idx,self.best_models = self.select_top_models(self.objects)\n",
    "        self.best_model_names = np.array(self.featnames)[self.best_idx]\n",
    "        \n",
    "        # getting all model predictions together for ensemble\n",
    "        if not self.test:\n",
    "            self.all_model_preds = [o.ypredvalid for o in self.best_models]\n",
    "            self.en = Ensemble(self.all_model_preds,self.y_valid)\n",
    "            self.precision = precision_score(self.y_valid,self.en.preds,labels=[3],average='micro')\n",
    "            \n",
    "        else:\n",
    "            self.all_model_preds = [o.yhattest for o in self.best_models]\n",
    "            self.en = Ensemble(self.all_model_preds)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def get_model_online(self,X_train,X_valid,y_train,y_valid,X_test=None):\n",
    "\n",
    "        if X_train.shape[1]<55:\n",
    "            pca_components = int(0.75*X_train.shape[1])\n",
    "        else:\n",
    "            pca_components=55\n",
    "            \n",
    "        obj = SVM(X_train,X_valid,y_train,y_valid,X_test,pca_comp=55,\n",
    "                           regC=30,kern='rbf',optimize=False,\n",
    "                           verbose=False,random_seed=self.random_seed)\n",
    "        \n",
    "        return obj\n",
    "    \n",
    "    \n",
    "    def get_model_offline(self,featfilename,featname,testfeatfilename=None):\n",
    "        \n",
    "        df1 = pd.read_csv(featfilename,header=None)\n",
    "        df2 = pd.read_csv(self.label_file,header=None)\n",
    "        df_feat = df1.merge(df2,on=0).set_index(0)\n",
    "        df_feat_train = df_feat.loc[self.enz_train]\n",
    "        df_feat_valid = df_feat.loc[self.enz_valid]\n",
    "        X_train_feat,y_train_feat = df_feat_train.iloc[:,0:-1].values,df_feat_train.iloc[:,-1].values\n",
    "        X_valid_feat,y_valid_feat = df_feat_valid.iloc[:,0:-1].values,df_feat_valid.iloc[:,-1].values\n",
    "        \n",
    "        pca_components = self.df_hyperparam.loc[featname,'pca_comp']\n",
    "        regCparam = self.df_hyperparam.loc[featname,'regC']\n",
    "        kernparam = self.df_hyperparam.loc[featname,'kernel']\n",
    "        \n",
    "        if X_train_feat.shape[1]<pca_components:\n",
    "            raise ValueError('Wrong Hyper Parameter')\n",
    "            \n",
    "        if self.test:\n",
    "            df_feat_test = pd.read_csv(testfeatfilename,header=None).set_index(0)\n",
    "            X_test_feat = df_feat_test.loc[self.testenz_names].values\n",
    "            if X_train_feat.shape[1] != X_test_feat.shape[1]:\n",
    "                print(featfilename)\n",
    "        else:\n",
    "            X_test_feat=None\n",
    "                \n",
    "        obj = SVM(X_train_feat,X_valid_feat,y_train_feat,y_valid_feat,X_test_feat,pca_comp=pca_components,\n",
    "                           regC=regCparam,kern=kernparam,optimize=False,\n",
    "                           verbose=False,random_seed=self.random_seed)\n",
    "        return obj\n",
    "        \n",
    "    def select_top_models(self,Os):\n",
    "        o_valid_accs = [o.acc_valid for o in Os] if self.test else [o.acc_train for o in Os] \n",
    "        sorted_idx = np.argsort(o_valid_accs)[::-1]\n",
    "        best_idx = sorted_idx[:self.n_models]\n",
    "        return best_idx,np.array(Os)[best_idx]\n",
    "\n",
    "    \n",
    "\n",
    "def get_precision(y,yhat):\n",
    "    return round(precision_score(y,yhat,labels=[3],average='micro'),2)\n",
    "\n",
    "\n",
    "def get_recall(y,yhat):\n",
    "    return round(recall_score(y,yhat,labels=[3],average='micro'),2)\n",
    "\n",
    "\n",
    "def get_f1_score(y,yhat):\n",
    "    return round(f1_score(y,yhat,average='micro'),2)\n",
    "\n",
    "def get_accuracy(y,yhat):\n",
    "    return round(accuracy_score(y,yhat),2)\n",
    "\n",
    "def write_model_stats(model_metrics):\n",
    "    datadict = {'precision':[],'recall':[],'f1_score':[],'accuracy':[]}\n",
    "    index_names = ['ensemble']\n",
    "    \n",
    "    precs = [m[0] for m in model_metrics]\n",
    "    recalls = [m[1] for m in model_metrics]\n",
    "    f1_scores = [m[2] for m in model_metrics]\n",
    "    accs = [m[3] for m in model_metrics]\n",
    "\n",
    "            \n",
    "    datadict['precision'].append(np.mean(precs))\n",
    "    datadict['recall'].append(np.mean(recalls))\n",
    "    datadict['f1_score'].append(np.mean(f1_scores))\n",
    "    datadict['accuracy'].append(np.mean(accs))\n",
    "\n",
    "    df = pd.DataFrame(datadict,index=index_names)\n",
    "    return df.to_csv(filename_model,mode='a',index=True,header=False)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 405 ms, sys: 144 ms, total: 549 ms\n",
      "Wall time: 52min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__=='__main__':\n",
    "    # Sequence and label files \n",
    "    enz_file = '../data/seq/EnzymeSequence.csv'\n",
    "    label_file = '../data/label/EnzymeLabelsMultiClass.csv'\n",
    "    \n",
    "    \n",
    "    hyperparam_file = '../data/results/HyperParameterOptimization/IndHPOpt.csv'\n",
    "\n",
    "    # Feature dir for iFeature,kernel,pssm \n",
    "    ifeatdatadir = '../featEngg/offline/ifeatMethods/data/featvec/trainfiles/'\n",
    "    kerneldatadir = '../featEngg/offline/kernelMethods/data/featvec/trainfiles/'\n",
    "    pssmdatadir = '../featEngg/offline/pssmMethods/data/featvec/trainfiles/'\n",
    "\n",
    "    trainfeatdirs = [ifeatdatadir,kerneldatadir,pssmdatadir]\n",
    "\n",
    "\n",
    "    \n",
    "    def multi_func(rs):\n",
    "        te_i = TEClassification(enz_file,None,label_file,trainfeatdirs,None,hyperparam_file,random_seed=rs)\n",
    "        yi = te_i.y_valid\n",
    "        yhati = te_i.en.preds\n",
    "        prec = get_precision(yi,yhati)\n",
    "        rec = get_recall(yi,yhati)\n",
    "        f1_score=get_f1_score(yi,yhati)\n",
    "        acc = get_accuracy(yi,yhati)\n",
    "        return prec,rec,f1_score,acc   \n",
    "    \n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    models = list(pool.map(multi_func,range(10000)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_model = '../data/results/feat_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_model_stats(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import multiprocessing as mp\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../scripts/')\n",
    "\n",
    "from MultiClassClassification import TEClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best base learner decider\n",
    "\n",
    "First step is to decide which base learning algorithm is suited to this problem. We train all the base learners in the ensemble using each of the three learning algorithms, SVM, NN and GBC and record their performance using the evaluation scheme discussed in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence and label files \n",
    "enz_file = '../data/seq/EnzymeSequence.csv'\n",
    "label_file = '../data/label/EnzymeLabelsMultiClass.csv'\n",
    "\n",
    "# Feature dir for iFeature,kernel,pssm \n",
    "ifeatdatadir = '../featEngg/offline/ifeatMethods/data/featvec/trainfiles/'\n",
    "kerneldatadir = '../featEngg/offline/kernelMethods/data/featvec/thymefiles1/train/'\n",
    "pssmdatadir = '../featEngg/offline/pssmMethods/data/featvec/trainfiles/'\n",
    "\n",
    "trainfeatdirs = [ifeatdatadir,kerneldatadir,pssmdatadir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_base_performance(rs,base_algo):\n",
    "    te = TEClassification(enz_file,None,label_file,trainfeatdirs,None,random_seed=rs, model=base_algo, optimize=False)\n",
    "    return te.precision, te.recall, te.en.acc\n",
    "\n",
    "def check_performance(base_algo):\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    N = 10000\n",
    "    iter_svm = zip(range(N),[base_algo for _ in range(N)])\n",
    "    metrics = pool.starmap(check_base_performance,iter_svm)\n",
    "    \n",
    "    precision = [m[0] for m in metrics]\n",
    "    recall = [m[1] for m in metrics]\n",
    "    accuracy = [m[2] for m in metrics]\n",
    "    \n",
    "    return round(np.mean(precision),2), round(np.mean(recall),2), round(np.mean(accuracy),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 433 ms, sys: 172 ms, total: 605 ms\n",
      "Wall time: 1h 2min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9, 0.92, 0.83)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "check_performance('SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 692 ms, sys: 249 ms, total: 940 ms\n",
      "Wall time: 1h 32min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.84, 0.94, 0.81)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "check_performance('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 576 ms, sys: 223 ms, total: 799 ms\n",
      "Wall time: 1h 22min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.84, 0.93, 0.81)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "check_performance('GBC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Hyperparameter Optimization\n",
    "\n",
    "After selecting the base learning algorithm selection, we hyperparameter optimize each base learner to get optimal results from the ensemble model. \n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "1. select base learner and simulate model with the optimize flag activated.\n",
    "2. train the ensemble and record the best hyperparameters learnt for each base model. \n",
    "3. run it 1000 times and record the hyperparameters for each base learner throughout the simulations.\n",
    "4. store the most frequent hyperparameters across the 1000 simulations for each base learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_hps(rs):\n",
    "    te = TEClassification(enz_file, None, label_file, trainfeatdirs, None, model='SVM', random_seed=rs, optimize=True)\n",
    "    return te.get_best_hps()\n",
    "\n",
    "\n",
    "def most_frequent(arr):\n",
    "    count = Counter(arr)\n",
    "    most_freq = count.most_common(1)[0][0]\n",
    "    return most_freq\n",
    "\n",
    "\n",
    "def store_best_hps(start=0):\n",
    "    N = 1000\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    all_hps = sum(list(tqdm.tqdm(pool.imap(best_hps, range(start,N+start)), total=N)),[])\n",
    "    all_hps = sorted(all_hps, key=lambda x: x[0])\n",
    "\n",
    "    with open('../data/results/hpopt/IndHPOpt.csv','w') as f:\n",
    "        f.write('feat_name,regC,kernel,pca_comp')\n",
    "        f.write('\\n')\n",
    "        for feat_name, feat_info in groupby(all_hps, key=lambda x: x[0]):\n",
    "            best_hp_list = [hp[1] for hp in list(feat_info)]\n",
    "            best_regC = most_frequent([x[0] for x in best_hp_list])\n",
    "            best_kernel = most_frequent([x[1] for x in best_hp_list])\n",
    "            best_ncomp = most_frequent([x[2] for x in best_hp_list])\n",
    "            f.write(f\"{feat_name},{best_regC},{best_kernel},{best_ncomp}\")\n",
    "            f.write('\\n')\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [6:02:14<00:00, 21.73s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.53 s, sys: 1.62 s, total: 9.15 s\n",
      "Wall time: 6h 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "store_best_hps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction technique performance\n",
    "\n",
    "Comparing the performance of individual feature extraction techniques.\n",
    "\n",
    "After getting the best set of hyperparameters, change the model such that it can accept the individual hyperparameter file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indhpoptfile = '../data/results/hpopt/IndHPOpt.csv'\n",
    "\n",
    "\n",
    "def get_precision(y,yhat,label=3):\n",
    "    return round(precision_score(y,yhat,labels=[label],average='micro'),2)\n",
    "\n",
    "def get_recall(y,yhat,label=3):\n",
    "    return round(recall_score(y,yhat,labels=[label],average='micro'),2)\n",
    "\n",
    "\n",
    "def get_accuracy(y,yhat):\n",
    "    return round(accuracy_score(y,yhat),2)\n",
    "\n",
    "def get_metrics(val_iter):\n",
    "    return get_precision(*val_iter), get_recall(*val_iter), get_accuracy(*val_iter)\n",
    "\n",
    "\n",
    "def get_validation_iter(obj):\n",
    "    return obj.yvalid,obj.ypredvalid\n",
    "\n",
    "\n",
    "def indfeat_performance(rs):\n",
    "    te = TEClassification(enz_file, None, label_file, trainfeatdirs, None, hyperparamfile=indhpoptfile, model='SVM', random_seed=rs, optimize=False)\n",
    "    val_iters = list(map(get_validation_iter,te.objects))\n",
    "    mets = list(map(get_metrics, val_iters))\n",
    "    return list(zip(te.featnames,mets))\n",
    "\n",
    "\n",
    "def indfeat_measure():\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    N = 10000\n",
    "    all_metrics = sum(list(tqdm.tqdm(pool.imap(indfeat_performance,range(N)), total=N)),[])\n",
    "    all_metrics = sorted(all_metrics, key=lambda x: x[0])\n",
    "    with open(\"../data/results/indfeatreport.csv\",'w') as f:\n",
    "        f.write('featname,min_precision,max_precision,mean_precision,std_precision,min_recall,max_recall,mean_recall,std_recall,min_accuracy,max_accuracy,mean_accuracy,std_accuracy')\n",
    "        f.write('\\n')\n",
    "        for featname, featinfo in groupby(all_metrics, key=lambda x:x[0]):\n",
    "            all_metric_list = [met[1] for met in featinfo]\n",
    "            all_prec = [m[0] for m in all_metric_list]\n",
    "            all_rec = [m[1] for m in all_metric_list]\n",
    "            all_acc = [m[2] for m in all_metric_list]\n",
    "\n",
    "            min_prec = round(min(all_prec),2)\n",
    "            max_prec = round(max(all_prec),2)\n",
    "            mean_prec = round(np.mean(all_prec),2)\n",
    "            std_prec = round(np.std(all_prec),2)\n",
    "\n",
    "            min_rec = round(min(all_rec),2)\n",
    "            max_rec = round(max(all_rec),2)\n",
    "            mean_rec = round(np.mean(all_rec),2)\n",
    "            std_rec = round(np.std(all_rec),2)\n",
    "\n",
    "            min_acc = round(min(all_acc),2)\n",
    "            max_acc = round(max(all_acc),2)\n",
    "            mean_acc = round(np.mean(all_acc),2)\n",
    "            std_acc = round(np.std(all_acc),2)\n",
    "            \n",
    "            f.write(f'{featname},{min_prec},{max_prec},{mean_prec},{std_prec},{min_rec},{max_rec},{mean_rec},{std_rec},{min_acc},{max_acc},{mean_acc},{std_acc}')\n",
    "            f.write('\\n')\n",
    "\n",
    "        \n",
    "    return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [59:21<00:00,  2.81it/s] \n"
     ]
    }
   ],
   "source": [
    "indfeat_measure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric sweep of ensemble model hyperparameter k\n",
    "\n",
    "With the individualized hyperparameters, its time to check the ensemble performance. We run a paramteric sweep of the ensemble model parameter k to get the best estimate of the parameter which denotes the number of base models to select in the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_param(k, rs):\n",
    "    te = TEClassification(enz_file, None, label_file, trainfeatdirs, None, hyperparamfile=indhpoptfile, model='SVM', random_seed=rs, n_models=k, optimize=False)\n",
    "    return te.y_valid,te.en.preds\n",
    "\n",
    "def ensemble_metrics(val_iter):\n",
    "    return get_metrics(val_iter)\n",
    "\n",
    "\n",
    "def ensemble_param_sweep(ks):\n",
    "    N = 10000\n",
    "    func_iter = list(itertools.product(ks, range(N)))\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    all_preds = list(pool.starmap(ensemble_param, func_iter))\n",
    "    all_metrics = list(map(get_metrics, all_preds))\n",
    "    \n",
    "    def get_mean(met):\n",
    "        return round(np.mean(met),2)\n",
    "    \n",
    "    with open('../data/results/en_param_sweep.csv','w') as f:\n",
    "        f.write('model_k,mean_precision,mean_recall,mean_accuracy')\n",
    "        f.write('\\n')\n",
    "        for model_k, model_info in itertools.groupby(zip(list(func_iter), all_metrics), key=lambda x:x[0][0]):\n",
    "            model_metrics = [m[1] for m in list(model_info)]\n",
    "            prec = [m[0] for m in model_metrics]\n",
    "            rec = [m[1] for m in model_metrics]\n",
    "            acc = [m[2] for m in model_metrics]    \n",
    "        \n",
    "            f.write(f\"{model_k},{','.join(list(map(str,(list(map(get_mean,[prec,rec,acc]))))))}\")\n",
    "            f.write('\\n')\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 409 ms, total: 1min 24s\n",
      "Wall time: 4h 54min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ensemble_param_sweep([5,9,15,21,31])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble model performance\n",
    "\n",
    "With the best base learning algorithm, the best sets of base learner hyperparameters, and the 5 top ranked feature extraction techniques, check the ensemble model performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble model parameter k=5 provides the best performance. Hence 5-base learners will be used in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_k</th>\n",
       "      <th>mean_precision</th>\n",
       "      <th>mean_recall</th>\n",
       "      <th>mean_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_k  mean_precision  mean_recall  mean_accuracy\n",
       "0        5            0.89         0.91           0.83\n",
       "1        9            0.89         0.90           0.82\n",
       "2       15            0.89         0.89           0.81\n",
       "3       21            0.89         0.89           0.81\n",
       "4       31            0.88         0.89           0.80"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../data/results/en_param_sweep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_preds(rs):\n",
    "    te = TEClassification(enz_file, None, label_file, trainfeatdirs, None, use_feat=None, hyperparamfile=indhpoptfile, model='SVM', random_seed=rs, n_models=5, optimize=False)\n",
    "    return te.y_valid,te.en.preds\n",
    "\n",
    "\n",
    "def ensemble_eval():\n",
    "    N = 10000\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    all_preds = list(tqdm.tqdm(pool.imap(ensemble_preds, range(N)), total=N))\n",
    "    model_metrics = list(map(get_metrics, all_preds))\n",
    "    all_prec = [m[0] for m in model_metrics]\n",
    "    all_rec = [m[1] for m in model_metrics]\n",
    "    all_acc = [m[2] for m in model_metrics]\n",
    "    \n",
    "    min_prec = round(min(all_prec),2)\n",
    "    max_prec = round(max(all_prec),2)\n",
    "    mean_prec = round(np.mean(all_prec),2)\n",
    "    std_prec = round(np.std(all_prec),2)\n",
    "\n",
    "    min_rec = round(min(all_rec),2)\n",
    "    max_rec = round(max(all_rec),2)\n",
    "    mean_rec = round(np.mean(all_rec),2)\n",
    "    std_rec = round(np.std(all_rec),2)\n",
    "\n",
    "    min_acc = round(min(all_acc),2)\n",
    "    max_acc = round(max(all_acc),2)\n",
    "    mean_acc = round(np.mean(all_acc),2)\n",
    "    std_acc = round(np.std(all_acc),2)\n",
    "    \n",
    "    with open('../data/results/ensemble_results.csv','w') as f:\n",
    "        f.write('ensemble,min_precision,max_precision,mean_precision,std_precision,min_recall,max_recall,mean_recall,std_recall,min_accuracy,max_accuracy,mean_accuracy,std_accuracy')\n",
    "        f.write('\\n')\n",
    "    \n",
    "        f.write(f'ensemble,{min_prec},{max_prec},{mean_prec},{std_prec},{min_rec},{max_rec},{mean_rec},{std_rec},{min_acc},{max_acc},{mean_acc},{std_acc}')\n",
    "        f.write('\\n')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [58:51<00:00,  2.83it/s] \n"
     ]
    }
   ],
   "source": [
    "ensemble_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation all three categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

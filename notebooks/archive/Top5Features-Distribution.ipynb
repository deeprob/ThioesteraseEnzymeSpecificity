{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top5 feats\n",
    "filename = '../data/results/feat_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools \n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score,f1_score\n",
    "\n",
    "sys.path.append('../')\n",
    "from baseModels.SVM.model import SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_model_report = '../data/results/feat_report.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DDE</th>\n",
       "      <td>0.895216</td>\n",
       "      <td>0.907472</td>\n",
       "      <td>0.806711</td>\n",
       "      <td>0.806711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTriad</th>\n",
       "      <td>0.888547</td>\n",
       "      <td>0.893962</td>\n",
       "      <td>0.793264</td>\n",
       "      <td>0.793264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KSCTriad</th>\n",
       "      <td>0.888547</td>\n",
       "      <td>0.893962</td>\n",
       "      <td>0.793264</td>\n",
       "      <td>0.793264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TPC</th>\n",
       "      <td>0.892713</td>\n",
       "      <td>0.892740</td>\n",
       "      <td>0.791088</td>\n",
       "      <td>0.791088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble</th>\n",
       "      <td>0.874912</td>\n",
       "      <td>0.891455</td>\n",
       "      <td>0.790993</td>\n",
       "      <td>0.790993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gappyKernel</th>\n",
       "      <td>0.880781</td>\n",
       "      <td>0.857368</td>\n",
       "      <td>0.787154</td>\n",
       "      <td>0.787154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             precision    recall  f1_score  accuracy\n",
       "DDE           0.895216  0.907472  0.806711  0.806711\n",
       "CTriad        0.888547  0.893962  0.793264  0.793264\n",
       "KSCTriad      0.888547  0.893962  0.793264  0.793264\n",
       "TPC           0.892713  0.892740  0.791088  0.791088\n",
       "ensemble      0.874912  0.891455  0.790993  0.790993\n",
       "gappyKernel   0.880781  0.857368  0.787154  0.787154"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('accuracy',ascending=False).head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feats = ['DDE','CTriad','KSCTriad','TPC','gappyKernel'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter optimization file\n",
    "hyperparam_file = '../data/results/HyperParameterOptimization/IndHPOpt.csv'\n",
    "\n",
    "df_hyperparam = pd.read_csv(hyperparam_file).set_index('feat_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label file\n",
    "label_file = '../data/label/EnzymeLabelsMultiClass.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifeat_feature_dir = '../featEngg/offline/ifeatMethods/data/featvec/trainfiles/'\n",
    "ifeat_feature_files = [ifeat_feature_dir+f.name for f in os.scandir(ifeat_feature_dir) if f.name.endswith('.csv.gz') and f.name.replace('.csv.gz','') in best_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_feature_dir = '../featEngg/offline/kernelMethods/data/featvec/trainfiles/'\n",
    "kernel_feature_files = [kernel_feature_dir+f.name for f in os.scandir(kernel_feature_dir) if f.name.endswith('.csv.gz') and f.name.replace('.csv.gz','') in best_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pssm_feature_dir = '../featEngg/offline/kernelMethods/data/featvec/trainfiles/'\n",
    "pssm_feature_files = [kernel_feature_dir+f.name for f in os.scandir(kernel_feature_dir) if f.name.endswith('.csv.gz') and f.name.replace('.csv.gz','') in best_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_files = ifeat_feature_files + kernel_feature_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_featname_from_file(filename):\n",
    "    pattern = re.compile(\"^(.+/)+(.+)\\.csv\\.gz$\")\n",
    "    m = re.match(pattern,filename)\n",
    "    return m.group(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(filename,random_seed):\n",
    "    featname = get_featname_from_file(filename)\n",
    "    \n",
    "    df1 = pd.read_csv(filename,header=None)\n",
    "    df2 = pd.read_csv(label_file,header=None)\n",
    "    df = df1.merge(df2,on=0)\n",
    "    enz_names = df[0].values\n",
    "    X = df.iloc[:,1:-1].values\n",
    "    y = df.iloc[:,-1].values\n",
    "    \n",
    "    pca_components = df_hyperparam.loc[featname,'pca_comp']\n",
    "    regCparam = df_hyperparam.loc[featname,'regC']\n",
    "    kernparam = df_hyperparam.loc[featname,'kernel']\n",
    "\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid,enz_train,enz_valid = train_test_split(X, y,enz_names, test_size=0.25, random_state=random_seed)\n",
    "    \n",
    "    if X_train.shape[1]<pca_components:\n",
    "        raise ValueError('Wrong Hyper Parameter')\n",
    "        \n",
    "    svm = SVM(X_train,X_valid,y_train,y_valid,verbose=False,optimize=False, pca_comp=pca_components,kern=kernparam,regC=regCparam)\n",
    "\n",
    "    yi = svm.yvalid\n",
    "    yhati = svm.ypredvalid\n",
    "    prec=get_precision(yi,yhati)\n",
    "    rec=get_recall(yi,yhati)\n",
    "    f1_score =get_f1_score(yi,yhati)\n",
    "    acc =get_accuracy(yi,yhati)\n",
    "    \n",
    "    return featname,prec,rec,f1_score,acc\n",
    "\n",
    "def get_precision(y,yhat):\n",
    "    return round(precision_score(y,yhat,labels=[3],average='micro'),2)\n",
    "\n",
    "\n",
    "def get_recall(y,yhat):\n",
    "    return round(recall_score(y,yhat,labels=[3],average='micro'),2)\n",
    "\n",
    "\n",
    "def get_f1_score(y,yhat):\n",
    "    return round(f1_score(y,yhat,average='micro'),2)\n",
    "\n",
    "def get_accuracy(y,yhat):\n",
    "    return round(accuracy_score(y,yhat),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count())\n",
    "iterable = list(itertools.product(feature_files,range(10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 380 ms, sys: 90.5 ms, total: 471 ms\n",
      "Wall time: 28min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "models = list(pool.starmap(model_evaluate,iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model_report(model_metrics):\n",
    "    datadict = {'min precision':[],'min recall':[],'min f1_score':[],'min accuracy':[],\n",
    "               'max precision':[],'max recall':[],'max f1_score':[],'max accuracy':[],\n",
    "               'mean precision':[],'mean recall':[],'mean f1_score':[],'mean accuracy':[],\n",
    "               'std precision':[],'std recall':[],'std f1_score':[],'std accuracy':[]}\n",
    "    \n",
    "    index_names = []\n",
    "    \n",
    "    for mn,group in itertools.groupby(model_metrics,key= lambda x:x[0]):\n",
    "        \n",
    "        index_names.append(mn)\n",
    "        \n",
    "        m_metrics = [g[1:] for g in list(group)]\n",
    "        \n",
    "        precs = [m[0] for m in m_metrics]\n",
    "        recalls = [m[1] for m in m_metrics]\n",
    "        f1_scores = [m[2] for m in m_metrics]\n",
    "        accs = [m[3] for m in m_metrics]\n",
    "\n",
    "\n",
    "        datadict['mean precision'].append(np.mean(precs))\n",
    "        datadict['mean recall'].append(np.mean(recalls))\n",
    "        datadict['mean f1_score'].append(np.mean(f1_scores))\n",
    "        datadict['mean accuracy'].append(np.mean(accs))\n",
    "\n",
    "        datadict['min precision'].append(min(precs))\n",
    "        datadict['min recall'].append(min(recalls))\n",
    "        datadict['min f1_score'].append(min(f1_scores))\n",
    "        datadict['min accuracy'].append(min(accs))\n",
    "\n",
    "        datadict['max precision'].append(max(precs))\n",
    "        datadict['max recall'].append(max(recalls))\n",
    "        datadict['max f1_score'].append(max(f1_scores))\n",
    "        datadict['max accuracy'].append(max(accs))\n",
    "\n",
    "        datadict['std precision'].append(np.std(precs))\n",
    "        datadict['std recall'].append(np.std(recalls))\n",
    "        datadict['std f1_score'].append(np.std(f1_scores))\n",
    "        datadict['std accuracy'].append(np.std(accs))\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame(datadict,index=index_names)\n",
    "    return df.to_csv(filename_model_report,mode='w',index=True,header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_model_report(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append('../')\n",
    "from ensemble.model import Ensemble\n",
    "from baseModels.SVM.model import SVM\n",
    "from featEngg.online.kmerMethods.models import ngModel,gaangModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEClassification:\n",
    "    \n",
    "    def __init__(self,enzseqdata,testenzseqdata,labelfile,trainfeaturefiledirs,testfeaturefiledirs,\n",
    "                 hyperparamfile,random_seed=None,n_models=17,validation_fraction=0.25):\n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        self.n_models = n_models\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.test = True if testfeaturefiledirs else False\n",
    "        \n",
    "                \n",
    "        # original data based on which everything is obtained\n",
    "        df1 = pd.read_csv(enzseqdata,header=None)\n",
    "        df2 = pd.read_csv(labelfile,header=None)\n",
    "        self.train_df = df1.merge(df2,on=0)\n",
    "        \n",
    "        self.enz_names = self.train_df[0].values\n",
    "        self.X = self.train_df.iloc[:,1].values\n",
    "        self.y = self.train_df.iloc[:,-1].values\n",
    "        \n",
    "        # training and validation data for general use\n",
    "        self.X_train, self.X_valid, self.y_train, self.y_valid,self.enz_train,self.enz_valid = train_test_split(self.X, self.y,self.enz_names, test_size=self.validation_fraction, random_state=self.random_seed)\n",
    "        \n",
    "        self.label_file = labelfile\n",
    "        \n",
    "        # test data\n",
    "        if self.test:\n",
    "            self.test_df = pd.read_csv(testenzseqdata,header=None)\n",
    "            self.testenz_names = self.test_df[0].values\n",
    "            self.X_test = self.test_df.iloc[:,1].values\n",
    "        else:\n",
    "            self.X_test=None\n",
    "            \n",
    "        self.df_hyperparam = pd.read_csv(hyperparamfile).set_index('feat_name')\n",
    "        \n",
    "        # kmer and gaakmer\n",
    "        ng = ngModel(self.X_train,self.X_valid,self.X_test)\n",
    "        gaang = gaangModel(self.X_train,self.X_valid,self.X_test)\n",
    "        kmernames = ['kmer','gaakmer']\n",
    "        kmerObjs = [self.get_model_online(ng.Xtrain,ng.Xvalid,self.y_train,self.y_valid,ng.Xtest),self.get_model_online(gaang.Xtrain,gaang.Xvalid,self.y_train,self.y_valid,gaang.Xtest)]\n",
    "\n",
    "        \n",
    "        #generate a list of names from the directories\n",
    "        trainfeatfiles = [d+f.name for d in trainfeaturefiledirs for f in os.scandir(d) if f.name.endswith('.csv.gz')]            \n",
    "        self.featnames = [f.name.replace('.csv.gz','') for d in trainfeaturefiledirs for f in os.scandir(d) if f.name.endswith('.csv.gz')]\n",
    "        \n",
    "        if self.test:\n",
    "            testfeatfiles = [d+f.name for d in testfeaturefiledirs for f in os.scandir(d) if f.name.endswith('.csv.gz')]\n",
    "            func_iter = list(zip(trainfeatfiles,self.featnames,testfeatfiles))\n",
    "            assert [f.name for d in trainfeaturefiledirs for f in os.scandir(d) if f.name.endswith('.csv.gz')]==[f.name for d in testfeaturefiledirs for f in os.scandir(d) if f.name.endswith('.csv.gz')]\n",
    "            self.objects=list(itertools.starmap(self.get_model_offline,func_iter))\n",
    "\n",
    "        else:\n",
    "            # getting all SVM objects together\n",
    "            func_iter = list(zip(trainfeatfiles,self.featnames))\n",
    "            self.objects = list(itertools.starmap(self.get_model_offline,func_iter))\n",
    "            \n",
    "        self.featnames.extend(kmernames)\n",
    "        self.objects.extend(kmerObjs)\n",
    "            \n",
    "        \n",
    "        # select only the best models based on training or validation\n",
    "        self.best_idx,self.best_models = self.select_top_models(self.objects)\n",
    "        self.best_model_names = np.array(self.featnames)[self.best_idx]\n",
    "        \n",
    "        # getting all model predictions together for ensemble\n",
    "        if not self.test:\n",
    "            self.all_model_preds = [o.ypredvalid for o in self.best_models]\n",
    "            self.en = Ensemble(self.all_model_preds,self.y_valid)\n",
    "            self.precision = precision_score(self.y_valid,self.en.preds,labels=[3],average='micro')\n",
    "            \n",
    "        else:\n",
    "            self.all_model_preds = [o.yhattest for o in self.best_models]\n",
    "            self.en = Ensemble(self.all_model_preds)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def get_model_online(self,X_train,X_valid,y_train,y_valid,X_test=None):\n",
    "\n",
    "        if X_train.shape[1]<55:\n",
    "            pca_components = int(0.75*X_train.shape[1])\n",
    "        else:\n",
    "            pca_components=55\n",
    "            \n",
    "        obj = SVM(X_train,X_valid,y_train,y_valid,X_test,pca_comp=55,\n",
    "                           regC=30,kern='rbf',optimize=False,\n",
    "                           verbose=False,random_seed=self.random_seed)\n",
    "        \n",
    "        return obj\n",
    "    \n",
    "    \n",
    "    def get_model_offline(self,featfilename,featname,testfeatfilename=None):\n",
    "        \n",
    "        df1 = pd.read_csv(featfilename,header=None)\n",
    "        df2 = pd.read_csv(self.label_file,header=None)\n",
    "        df_feat = df1.merge(df2,on=0).set_index(0)\n",
    "        df_feat_train = df_feat.loc[self.enz_train]\n",
    "        df_feat_valid = df_feat.loc[self.enz_valid]\n",
    "        X_train_feat,y_train_feat = df_feat_train.iloc[:,0:-1].values,df_feat_train.iloc[:,-1].values\n",
    "        X_valid_feat,y_valid_feat = df_feat_valid.iloc[:,0:-1].values,df_feat_valid.iloc[:,-1].values\n",
    "        \n",
    "        pca_components = self.df_hyperparam.loc[featname,'pca_comp']\n",
    "        regCparam = self.df_hyperparam.loc[featname,'regC']\n",
    "        kernparam = self.df_hyperparam.loc[featname,'kernel']\n",
    "        \n",
    "        if X_train_feat.shape[1]<pca_components:\n",
    "            raise ValueError('Wrong Hyper Parameter')\n",
    "            \n",
    "        if self.test:\n",
    "            df_feat_test = pd.read_csv(testfeatfilename,header=None).set_index(0)\n",
    "            X_test_feat = df_feat_test.loc[self.testenz_names].values\n",
    "            if X_train_feat.shape[1] != X_test_feat.shape[1]:\n",
    "                print(featfilename)\n",
    "        else:\n",
    "            X_test_feat=None\n",
    "                \n",
    "        obj = SVM(X_train_feat,X_valid_feat,y_train_feat,y_valid_feat,X_test_feat,pca_comp=pca_components,\n",
    "                           regC=regCparam,kern=kernparam,optimize=False,\n",
    "                           verbose=False,random_seed=self.random_seed)\n",
    "        return obj\n",
    "        \n",
    "    def select_top_models(self,Os):\n",
    "        o_valid_accs = [o.acc_valid for o in Os] #if self.test else [o.acc_train for o in Os] \n",
    "        sorted_idx = np.argsort(o_valid_accs)[::-1]\n",
    "        best_idx = sorted_idx[:self.n_models]\n",
    "        return best_idx,np.array(Os)[best_idx]\n",
    "\n",
    "    \n",
    "\n",
    "def get_precision(y,yhat):\n",
    "    return round(precision_score(y,yhat,labels=[3],average='micro'),2)\n",
    "\n",
    "\n",
    "def get_recall(y,yhat):\n",
    "    return round(recall_score(y,yhat,labels=[3],average='micro'),2)\n",
    "\n",
    "\n",
    "def get_f1_score(y,yhat):\n",
    "    return round(f1_score(y,yhat,average='micro'),2)\n",
    "\n",
    "def get_accuracy(y,yhat):\n",
    "    return round(accuracy_score(y,yhat),2)\n",
    "\n",
    "def write_model_stats(model_metrics):\n",
    "    datadict = {'precision':[],'recall':[],'f1_score':[],'accuracy':[]}\n",
    "    index_names = ['ensemble']\n",
    "    \n",
    "    precs = [m[0] for m in model_metrics]\n",
    "    recalls = [m[1] for m in model_metrics]\n",
    "    f1_scores = [m[2] for m in model_metrics]\n",
    "    accs = [m[3] for m in model_metrics]\n",
    "\n",
    "            \n",
    "    datadict['precision'].append(np.mean(precs))\n",
    "    datadict['recall'].append(np.mean(recalls))\n",
    "    datadict['f1_score'].append(np.mean(f1_scores))\n",
    "    datadict['accuracy'].append(np.mean(accs))\n",
    "\n",
    "    df = pd.DataFrame(datadict,index=index_names)\n",
    "    return df.to_csv(filename_model,mode='a',index=True,header=False)       \n",
    "\n",
    "def write_model_report(model_metrics):\n",
    "    datadict = {'min precision':[],'min recall':[],'min f1_score':[],'min accuracy':[],\n",
    "               'max precision':[],'max recall':[],'max f1_score':[],'max accuracy':[],\n",
    "               'mean precision':[],'mean recall':[],'mean f1_score':[],'mean accuracy':[],\n",
    "               'std precision':[],'std recall':[],'std f1_score':[],'std accuracy':[]}\n",
    "    \n",
    "    index_names = ['ensemble']\n",
    "    \n",
    "    precs = [m[0] for m in model_metrics]\n",
    "    recalls = [m[1] for m in model_metrics]\n",
    "    f1_scores = [m[2] for m in model_metrics]\n",
    "    accs = [m[3] for m in model_metrics]\n",
    "\n",
    "            \n",
    "    datadict['mean precision'].append(np.mean(precs))\n",
    "    datadict['mean recall'].append(np.mean(recalls))\n",
    "    datadict['mean f1_score'].append(np.mean(f1_scores))\n",
    "    datadict['mean accuracy'].append(np.mean(accs))\n",
    "    \n",
    "    datadict['min precision'].append(min(precs))\n",
    "    datadict['min recall'].append(min(recalls))\n",
    "    datadict['min f1_score'].append(min(f1_scores))\n",
    "    datadict['min accuracy'].append(min(accs))\n",
    "    \n",
    "    datadict['max precision'].append(max(precs))\n",
    "    datadict['max recall'].append(max(recalls))\n",
    "    datadict['max f1_score'].append(max(f1_scores))\n",
    "    datadict['max accuracy'].append(max(accs))\n",
    "    \n",
    "    datadict['std precision'].append(np.std(precs))\n",
    "    datadict['std recall'].append(np.std(recalls))\n",
    "    datadict['std f1_score'].append(np.std(f1_scores))\n",
    "    datadict['std accuracy'].append(np.std(accs))\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame(datadict,index=index_names)\n",
    "    return df#.to_csv(filename_model_report,mode='a',index=True,header=False)    \n",
    "\n",
    "\n",
    "def save_model_metrics(metrics):\n",
    "    filename = '../data/results/EnsembleResults/MultiClassEnsemble.csv'\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        for i,j,k,l in metrics:\n",
    "            f.write(f'{i},{j},{k},{l}')\n",
    "            f.write('\\n')\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 588 ms, sys: 187 ms, total: 775 ms\n",
      "Wall time: 53min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__=='__main__':\n",
    "    # Sequence and label files \n",
    "    enz_file = '../data/seq/EnzymeSequence.csv'\n",
    "    label_file = '../data/label/EnzymeLabelsMultiClass.csv'\n",
    "    \n",
    "    \n",
    "    hyperparam_file = '../data/results/HyperParameterOptimization/IndHPOpt.csv'\n",
    "\n",
    "    # Feature dir for iFeature,kernel,pssm \n",
    "    ifeatdatadir = '../featEngg/offline/ifeatMethods/data/featvec/trainfiles/'\n",
    "    kerneldatadir = '../featEngg/offline/kernelMethods/data/featvec/trainfiles/'\n",
    "    pssmdatadir = '../featEngg/offline/pssmMethods/data/featvec/trainfiles/'\n",
    "\n",
    "    trainfeatdirs = [ifeatdatadir,kerneldatadir,pssmdatadir]\n",
    "\n",
    "\n",
    "    \n",
    "    def multi_func(rs):\n",
    "        te_i = TEClassification(enz_file,None,label_file,trainfeatdirs,None,hyperparam_file,random_seed=rs,n_models=5)\n",
    "        yi = te_i.y_valid\n",
    "        yhati = te_i.en.preds\n",
    "        prec = get_precision(yi,yhati)\n",
    "        rec = get_recall(yi,yhati)\n",
    "        f1_score=get_f1_score(yi,yhati)\n",
    "        acc = get_accuracy(yi,yhati)\n",
    "        return prec,rec,f1_score,acc   \n",
    "    \n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    models = list(pool.map(multi_func,range(10000)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_model = '../data/results/feat_results.csv'\n",
    "filename_model_report = '../data/results/feat_report.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min precision</th>\n",
       "      <th>min recall</th>\n",
       "      <th>min f1_score</th>\n",
       "      <th>min accuracy</th>\n",
       "      <th>max precision</th>\n",
       "      <th>max recall</th>\n",
       "      <th>max f1_score</th>\n",
       "      <th>max accuracy</th>\n",
       "      <th>mean precision</th>\n",
       "      <th>mean recall</th>\n",
       "      <th>mean f1_score</th>\n",
       "      <th>mean accuracy</th>\n",
       "      <th>std precision</th>\n",
       "      <th>std recall</th>\n",
       "      <th>std f1_score</th>\n",
       "      <th>std accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ensemble</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.898873</td>\n",
       "      <td>0.916701</td>\n",
       "      <td>0.829638</td>\n",
       "      <td>0.829638</td>\n",
       "      <td>0.081997</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>0.06185</td>\n",
       "      <td>0.06185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          min precision  min recall  min f1_score  min accuracy  \\\n",
       "ensemble            0.5         0.5          0.55          0.55   \n",
       "\n",
       "          max precision  max recall  max f1_score  max accuracy  \\\n",
       "ensemble            1.0         1.0           1.0           1.0   \n",
       "\n",
       "          mean precision  mean recall  mean f1_score  mean accuracy  \\\n",
       "ensemble        0.898873     0.916701       0.829638       0.829638   \n",
       "\n",
       "          std precision  std recall  std f1_score  std accuracy  \n",
       "ensemble       0.081997    0.078663       0.06185       0.06185  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_model_report(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_metrics(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_model_stats(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
